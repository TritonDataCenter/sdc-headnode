#!/bin/bash
#
# Copyright (c) 2013, Joyent, Inc. All rights reserved.
#

#
# We set errexit (a.k.a. "set -e") to force an exit on error conditions, but
# there are many important error conditions that this does not capture --
# first among them failures within a pipeline (only the exit status of the
# final stage is propagated).  To exit on these failures, we also set
# "pipefail" (a very useful option introduced to bash as of version 3 that
# propagates any non-zero exit values in a pipeline).
#

set -o errexit
set -o pipefail

shopt -s extglob

ROOT=$(cd $(dirname $0)/../; pwd)

# Write output to log file.
THIS_TIMESTAMP=${TIMESTAMP}
if [[ -z "$THIS_TIMESTAMP" ]]; then
    THIS_TIMESTAMP=$(date -u "+%Y%m%dT%H%M%SZ")
fi
LOGDIR="${ROOT}/log"
LOGFILE="${LOGDIR}/build.log.${THIS_TIMESTAMP}"
RONNJS="${ROOT}/bin/ronnjs/bin/ronn.js"

mkdir -p log
exec > >(tee ${LOGFILE}) 2>&1

if [[ $(echo $BASH_VERSION | cut -d '.' -f1-2) > 4.0 ]]; then
    BASH_IS_NOT_ANCIENT='true'
fi
#export PS4='${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
#set -x
if [[ `hostname` == "bh1-autobuild" || `hostname` == "bldzone2.joyent.us" || ! -z $BASH_IS_NOT_ANCIENT ]]; then
    export PS4='[\D{%FT%TZ}] ${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
    export BASH_XTRACEFD=4
    set -o xtrace
fi


# Tools.
AWK=$((which gawk 2>/dev/null | grep -v "^no ") || which awk)
TAR=tar
GREP=grep
if [[ `uname -s` == 'SunOS' ]]; then
  SUM='/usr/bin/sum -x sha1'
else
  SUM='shasum'
fi


# See MGs Package Versioning for details (https://mo.joyent.com/mountain-gorilla/blob/master/README.md#L74).
THIS_BRANCH=$(git symbolic-ref HEAD | cut -d'/' -f3)
THIS_GITDESCRIBE=g$(git describe --all --long | $AWK -F'-g' '{print $NF}')
THIS_BUILDSTAMP=${THIS_BRANCH}-${THIS_TIMESTAMP}-${THIS_GITDESCRIBE}

# "SDC_VERSION" is the version value that gets exposed to the public
# for development builds this will read <ts>.<branch>.<sha> of the build
# this value ends up in /usbkey/sdc_version
if [[ -z $SDC_VERSION ]]; then
  SDC_VERSION=${THIS_BUILDSTAMP}
fi

echo ">> Starting build at $(date)"

function fatal
{
    echo "$(basename $0): fatal error: $*"
    exit 1
}

function errexit
{
    [[ $1 -ne 0 ]] || exit 0
    fatal "error exit status $1 at line $2"
}

function check_nodejs
{
    [[ ! `which node` ]] && fatal "build-image requires node to be in your path"

    ver=`node --version`
    micro=${ver##*.}
    front=${ver%.*}
    minor=${front##*.}

    # [[ $minor -ne 4 ]] && fatal "Node minor version must be 4"
    # [[ $micro -lt 9 ]] && fatal "Node micro version must be at least 9"

    if [[ $(echo '{"foo": "bar"}' | ${ROOT}/bin/json foo) == 'bar' ]]; then
        echo "Your version of node.js is ok!"
    else
        fatal "You need to have a working node.js installed for this to work!"
    fi
}

MERGED_SPEC=
if [[ -f "${ROOT}/build.spec" && -f "${ROOT}/build.spec.local" ]]; then
    MERGED_SPEC=$(${ROOT}/bin/json-merge ${ROOT}/build.spec ${ROOT}/build.spec.local);
elif [[ -f "${ROOT}/build.spec" ]]; then
    MERGED_SPEC=$(cat ${ROOT}/build.spec);
elif [[ -f "${ROOT}/build.spec.local" ]]; then
    MERGED_SPEC=$(cat ${ROOT}/build.spec.local);
fi

function build_spec () {
    local thing=$1;
    echo $(echo $MERGED_SPEC | ${ROOT}/bin/json ${thing});
};

trap 'errexit $? $LINENO' EXIT

STAGE="${ROOT}/cache/stage"
ERROR=0
CLEANED=0

CURL_OPTS=$(build_spec curl-opts)
SPEED_LIMIT=$(build_spec speed-limit)
NO_INTERNET=$(build_spec no-internet)

if [[ -n ${SPEED_LIMIT} ]]; then
    CURL_OPTS="${CURL_OPTS} --limit-rate ${SPEED_LIMIT}"
fi

# Determine BITS_URL, BITS_BRANCH and BITS_DIR. These determine where this
# build will get its dependent pre-built bits.
#
# "BITS_URL" can either be a URL directory, e.g.
# "https://stuff.joyent.us/stuff/builds", or a local
# directory. If an existing local dir, then "BITS_DIR" will be set to that
# for the build code below.
#
# If BITS_URL is a URL, it is expected to be of the following structure:
#       $BITS_URL/
#           agentsshar/
#               $branch-$date1/
#               ...
#               $branch-latest/
#                   AGENTS SHAR PACKAGE
#           ca/
#               $branch-$date1/
#               ...
#               $branch-latest/
#                   CA ZONE FS TARBALL PACKAGE
#           amon/
#               $branch-$date1/
#               ...
#               $branch-latest/
#                   AMON ZONE FS TARBALL PACKAGE
#           ... likewise for ufds and platform.
#
# This is the build structure created by the SDC "mountain-gorilla" build
# system that is being run by Jenkins CI
# (https://hub.joyent.com/wiki/display/dev/Jenkins) and uploaded
# to <https://stuff.joyent.us/stuff/builds>.
#
# As well, if BITS_URL is a URL, then BITS_BRANCH must also be specified.
#
# Ways to specify BITS_URL:
# - BITS_URL envvar
# - MASTER_PLATFORM_URL envvar (deprecated, backward-compat)
# - "bits-url" entry in build.spec.local
# - "master-url" entry in build.spec.local (deprecated, backward-compat)
# - "bits-url" entry in build.spec
#
# Ways to specify BITS_BRANCH:
# - BITS_BRANCH envvar
# - "bits-branch" in build.spec.local
# - "bits-branch" in build.spec

# Get BITS_URL.
if [[ -z "$BITS_URL" && ! -z "$MASTER_PLATFORM_URL" ]]; then
    echo "WARNING: using 'MASTER_PLATFORM_URL' envvar is deprecated, use 'BITS_URL' instead"
    BITS_URL=$MASTER_PLATFORM_URL
fi
if [[ -z "$BITS_URL" ]]; then
    BITS_URL=$(build_spec bits-url)
    if [[ ! -f ~/.sdcbuild.json ]]; then
      (scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o \
        BatchMode=yes stuff@stuff.joyent.us:.sdcbuild.json ~/.sdcbuild.json || /usr/bin/true)
    fi
    if [[ -f ~/.sdcbuild.json ]]; then
      username=$(cat ~/.sdcbuild.json | ${ROOT}/bin/json username)
      password=$(cat ~/.sdcbuild.json | ${ROOT}/bin/json password)
      BITS_URL=$(echo $BITS_URL | sed -e "s/\/\//\/\/${username}:${password}@/g")
    else
      BITS_URL=$(build_spec bits-url-open)
    fi
fi
if [[ -z "$BITS_URL" && ! -z "$(build_spec master-url)" ]]; then
    echo "WARNING: the 'master-url' key in build.spec.local is deprecated, use 'bits-url' instead"
    BITS_URL=$(build_spec master-url)
fi
[[ -z "$BITS_URL" ]] && fatal "Could not determine a BITS_URL."

# Validate BITS_URL and set BITS_DIR if it is a dir.
if [[ "${BITS_URL:0:8}" == "https://" || "${BITS_URL:0:7}" == "http://" ]]; then
    BITS_URL=$BITS_URL
    BITS_DIR=
elif [[ -d "${BITS_URL}" ]]; then
    BITS_DIR=$BITS_URL
    echo "BITS_DIR (BITS_URL is a local dir): $BITS_DIR"
else
    fatal "BITS_URL ($BITS_URL) is not a known protocol or an existing dir"
fi

# Get BITS_BRANCH if necessary.
if [[ -z "$BITS_DIR" ]]; then
    if [[ -z "$BITS_BRANCH" ]]; then
        BITS_BRANCH=$(build_spec bits-branch)
    fi

    [[ -z "$BITS_BRANCH" ]] && fatal "Could not determine a BITS_BRANCH."
    echo "BITS_BRANCH: $BITS_BRANCH"
fi

# See <https://hub.joyent.com/wiki/display/doc/Special+CAPI+Accounts> for
# details on user used with DSAPI.
DSAPI_URL="https://usbheadnode:shnek7bi3op5@datasets.joyent.com"

if [[ $1 == "-r" ]]; then
    # XXX - Temporary warning about recipes
    echo "WARNING: Recipes are no longer supported... sleeping for while so you notice"
    sleep 30
    shift
    shift
fi

if [[ $1 == "-c" ]]; then
    shift
    echo "NOTICE: building without config is the only option now, -c is unnecessary."
fi

PLATFORM=$(uname -s)
if [[ ${PLATFORM} == 'Darwin' || ${PLATFORM} == 'SunOS' ]]; then
    source ${ROOT}/bin/include-tar-generic
    version
else
    echo "FATAL: Unsupported platform '${PLATFORM}'"
fi

echo -n "==> Checking for Internets... "
if [[ ${NO_INTERNET} == "true" ]] || ! can_has_internets; then
    echo "No Internets! Activating countermeasures!"
    HAVE_INTERNET="false"
else
    echo "Yep!"
    HAVE_INTERNET="true"
fi

function test_rootperms
{
    # root access is no longer required on OSX
    [[ ${PLATFORM} == 'Darwin' ]] && return
    su_uid=$(${SUCMD} id -u)
    if [[ ${su_uid} -ne 0 ]]; then
        fatal "Can't get root priviledges."
    fi
}

function load_buildspec
{
    PLATFORM_RELEASE=$(build_spec platform-release)
    BUILD_TGZ=$(build_spec build-tgz)

    # HEAD-1507 cleanup.
    USE_IMAGES=$(build_spec use-images)
    [[ ${USE_IMAGES} == "false" ]] && USE_IMAGES=''

    [[ -n ${PLATFORM_RELEASE} ]] && echo "platform-release: ${PLATFORM_RELEASE}"
}

function create_directories
{
    if [ ! -d "${ROOT}/cache" ]; then
        echo "==> Creating cache/"
        mkdir -p ${ROOT}/cache
    fi

    if [ ! -d "${ROOT}/mnt" ]; then
        echo "==> Creating mnt/"
        mkdir -p ${ROOT}/mnt
    fi

    echo "==> Creating stage/"
    rm -rf ${STAGE}
    mkdir -p ${STAGE}
    mkdir -p ${STAGE}/extra/pkgsrc
    mkdir -p ${STAGE}/extra/amon-agent
}

function generate_grub_menu
{
    local unit=
    local serial_dev=$(build_spec serial-dev)
    local console=$(build_spec console)
    local default_boot_option=$(build_spec default-boot-option)

    echo "==> Generating grub menu"

    [[ -z "${serial_dev}" ]] && serial_dev=ttyb
    [[ -z "${console}" ]] && console="serial"
    [[ -z "${default_boot_option}" ]] && default_boot_option=0

    #
    # This section describes the serial-dev and console parameters.  These
    # values may be overridden in build.spec{,.local}.
    #
    # serial-dev is the serial console device on the target system.  It
    # defaults to ttyb (illumos) aka COM2 (FreeDOS), which is legacy I/O
    # port 2f8 interrupt 3.
    #
    # console is used to set the default value of the GRUB variable
    # "os_console", which selects the post-boot console device.  It may
    # be one of "serial", in which case the serial device specified by
    # serial-dev is used, or "text" in which case an attached keyboard
    # and VGA device is used.  For backward compatibility, "graphics"
    # and "vga" are aliases for "text".  In addition, an explicit serial
    # device may be specified, in which case its value will override
    # serial-dev.  This is almost certainly not what you want, since it
    # will mean that post-boot I/O will be to/from a different device
    # than was used during boot.  Note that the operator can change the
    # post-boot console by modifying the os_console GRUB variable before
    # booting.
    #
    # When the system boots, GRUB will display its output to the VGA
    # device, if one is present, and the device specified by serial-dev,
    # if it exists.  It will also accept input from either an attached
    # keyboard or serial-dev.  Once a boot selection is made, the value
    # of the os_console GRUB variable is passed to the operating system
    # and used as the system console, unless a network boot is
    # performed.  In that case, the parameters received from the HN will
    # override all console selection made here or in the GRUB
    # environment; this may be modified for each CN using CNAPI.
    #
    # By default, serial-dev is "ttyb" and console is "serial".
    #
    case "${serial_dev}" in
    ttya)
        unit=0
        ;;
    ttyb)
        unit=1
        ;;
    ttyc)
        unit=2
        ;;
    ttyd)
        unit=3
        ;;
    *)
        fatal "Unknown serial device: ${serial_dev}"
        ;;
    esac

    case "${console}" in
    serial)
        console=${serial_dev}
        ;;
    ttya|ttyb|ttyc|ttyd)
        ;;
    text|graphics|vga)
        console=text
        ;;
    *)
        fatal "Unknown default console device: ${console}"
        ;;
    esac

    serial_string="--speed=115200 --unit=${unit} --word=8 --parity=no --stop=1"

    sed \
        -e "s/^#SERIAL/serial ${serial_string}/" \
        -e "s/DEFAULT_CONSOLE/${console}/g" \
        -e "s/^default.*$/default ${default_boot_option}/" \
        boot/grub/menu.lst.tmpl \
        > ${STAGE}/boot/grub/menu.lst.tmpl
}

function copy_base
{
    local sbbranch=$(build_spec sdcboot-release)
    local platbranch=$(build_spec sdcboot-release)
    local ftbranch=$(build_spec firmware-tools-release)

    [[ -z ${sbbranch} ]] && sbbranch="master"
    [[ -z ${platbranch} ]] && platbranch="master"
    [[ -z ${ftbranch} ]] && ftbranch="master"

    local sdcboot_path=$(get_bit sdcboot/sdcboot-${sbbranch}-.*\.tgz)
    local platboot_path=$(get_bit platform/boot-${platbranch}-.*\.tgz)
    local firmware_path=$(get_bit \
	firmware-tools/firmware-tools-${ftbranch}-.*\.tgz)

    echo "==> Creating .joyliveusb file"
    touch ${STAGE}/.joyliveusb

    echo "==> Copying in scripts/"
    cp -r scripts ${STAGE}/scripts

    echo "==> Copying in zones/"
    cp -r zones ${STAGE}/zones

    echo "==> Copying in default/"
    cp -r default ${STAGE}/default

    echo "==> Copying in rc/"
    cp -r rc ${STAGE}/rc

    echo "==> Copying in EULA"
    cp -r EULA ${STAGE}/EULA

    echo "==> Extracting platform boot bundle"
    (cd ${STAGE} && ${TAR} xzf ${platboot_path})

    echo "==> Extracting sdcboot bundle"
    (cd ${STAGE} && ${TAR} xzf ${sdcboot_path})

    echo "==> Extracting firmware bundle"
    (cd ${STAGE} && ${TAR} xzf ${firmware_path})
}

function copy_config {

    # Clear current configs from stage area
    rm -f ${STAGE}/config || true
    rm -rf ${STAGE}/config.inc || true

    cp -r config/config.inc ${STAGE}/config.inc

    if [[ -f config/banner ]]; then
        cp config/banner ${STAGE}/banner
    fi
}

function pkgin_get
{
    local url=$1
    local file=$2
    local dest=$3
    local ver=$(echo ${url} | cut -d '/' -f5-7)

    if [[ -n $PKGSRC_DIR ]]; then
        if [[ ${file} == "SHA512.bz2" ]]; then
            # special case, since this file is in a weird spot
            cp ${PKGSRC_DIR}/${ver}/../${file} ${dest}
        else
            cp ${PKGSRC_DIR}/${ver}/${file} ${dest}
        fi
    else
        (cd ${dest} && curl -k -f0 ${CURL_OPTS} -O ${url}/${file})
    fi
}

# HEAD-1507 obsolete.
function copy_pkgsrc
{
    [[ -z ${USE_IMAGES} ]] || return 0

    echo "==> Copying in pkgsrc"

    # move old cache to new name so everyone doesn't have to download again.
    [[ ! -d ${ROOT}/cache/2010Q4 && -d ${ROOT}/cache/pkgsrc_2010Q4 ]] && \
        mv -f ${ROOT}/cache/pkgsrc_2010Q4 ${ROOT}/cache/2010Q4
    [[ ! -d ${ROOT}/cache/2011Q2_64 && -d ${ROOT}/cache/pkgsrc_2011Q2_64 ]] && \
        mv -f ${ROOT}/cache/pkgsrc_2011Q2_64 ${ROOT}/cache/2011Q2_64
    [[ ! -d ${ROOT}/cache/2011Q4 && -d ${ROOT}/cache/pkgsrc_2011Q4 ]] && \
        mv -f ${ROOT}/cache/pkgsrc_2011Q4 ${ROOT}/cache/2011Q4
    [[ ! -d ${ROOT}/cache/2011Q4_64 && -d ${ROOT}/cache/pkgsrc_2011Q4_64 ]] && \
        mv -f ${ROOT}/cache/pkgsrc_2011Q4_64 ${ROOT}/cache/2011Q4_64

    PKGSRC_LIST=
    local data=$((${ROOT}/bin/json datasets \
        | ${ROOT}/bin/json -a name pkgsrc pkgsrc_url | tr ' ' '|' \
        | grep -v "|$") < ${ROOT}/build.spec)
    local idx=0
    rm -f ${ROOT}/cache/*.lst
    for ds in ${data}; do
        local ds_name=$(echo "${ds}" | cut -d '|' -f1)
        local ds_pkgsrc=$(echo "${ds}" | cut -d '|' -f2)
        local ds_pkgsrc_url=$(echo "${ds}" | cut -d '|' -f3)

        for zone in $(ls ${ROOT}/zones); do
            zoneds=$(cat ${ROOT}/zones/${zone}/dataset)
            if [[ ${zoneds} =~ ${ds_name} ]]; then
                cat ${ROOT}/zones/${zone}/pkgsrc >> ${ROOT}/cache/${ds_pkgsrc}.lst
                mkdir -p ${STAGE}/zones/${zone}
            fi
        done
        if [[ -f ${ROOT}/cache/${ds_pkgsrc}.lst ]]; then
            # Someone needs this pkgsrc
            PKGSRC_LIST[${idx}]="${ds_pkgsrc}|${ds_pkgsrc_url}"
            idx=$((${idx} + 1))
        fi
    done

    for pkgsrc in ${PKGSRC_LIST[@]} ; do
        local name=${pkgsrc%%|*}
        local url=${pkgsrc##*|}
        local do_download=0

        local pkglist=$(find ${ROOT}/cache -type f -iname ${name}.lst)
        [[ -z ${pkglist} ]] && continue

        local cache_dir=${ROOT}/cache/${name}
        mkdir -p ${cache_dir}

        # XXX:
        #
        # We used to always have md5sums.txt in pkgsrc, since DATASET-27. For
        # the new "multiarch" pkgsrc we no longer have these.  Instead per
        # DATASET-647 we're supposed to use a new SHA512.bz2 file.  For now
        # we need to support both with a preference for SHA512.bz2 when it
        # exists.  When we no longer support old pkgsrc repos with md5sum.txt
        # we can remove the MD5 checks below.
        #

        if [[ ${HAVE_INTERNET} == "true" || -n ${PKGSRC_DIR} ]]; then
            (cd ${cache_dir} \
                && rm -f md5sums.txt SHA512.bz2;
                if ! pkgin_get ${url}/../ SHA512.bz2 ./; then
                    if ! pkgin_get ${url} md5sums.txt ./; then
                        fatal "Failed to download either SHA512.bz2 or md5sums.txt"
                    fi
                fi
            )
        elif [[ ! -f ${cache_dir}/md5sums.txt && ! -f ${cache_dir}/SHA512.bz2 ]]; then
            fatal "Don't have cached ${cache_dir}/md5sums.txt or" \
                "${cache_dir}/SHA512.bz2 files, can't build." \
                "You need to find some Internet."
        fi

        pkgs=$(cat ${pkglist} \
            | xargs -n1 \
            | sort \
            | uniq \
            | sed -e "s/$/.tgz/")
        for pkgfile in $pkgs; do
            if [[ -f ${cache_dir}/md5sums.txt ]]; then
                MD5=$(${GREP} " ${pkgfile}" ${cache_dir}/md5sums.txt | cut -d' ' -f1 || true)
            else
                MD5=
            fi
            if [[ -f ${cache_dir}/SHA512.bz2 ]]; then
                SHA512=$(bzcat ${cache_dir}/SHA512.bz2 | ${GREP} "/${pkgfile})" | cut -d' ' -f4 || true)
            else
                SHA512=
            fi
            if [[ -z ${SHA512} && -z ${MD5} ]]; then
                fatal "Unable to find md5sum or SHA512 for ${pkgfile}, " \
                    "must be fixed before we can continue."
            fi

            if [[ -f ${cache_dir}/${pkgfile} ]]; then
                [[ -n ${MD5} ]] && ACTUAL_MD5=$(openssl dgst -md5 ${cache_dir}/${pkgfile} | awk '{print $NF}')
                [[ -n ${SHA512} ]] && ACTUAL_SHA512=$(openssl dgst -sha512 ${cache_dir}/${pkgfile} | awk '{print $NF}')
            fi

            if [[ ! -f ${cache_dir}/${pkgfile} ]]; then
                # don't have it at all, download
                do_download=1
            elif [[ -n ${SHA512} ]]; then
                if [[ -z ${ACTUAL_SHA512} || ${SHA512} != ${ACTUAL_SHA512} ]]; then
                    # don't have a good file, download now
                    do_download=1
                fi
            elif [[ -n ${MD5} ]]; then
                if [[ -z ${ACTUAL_MD5} || ${MD5} != ${ACTUAL_MD5} ]]; then
                    # don't have a good file, download now
                    do_download=1
                fi
            fi

            if [[ ${do_download} == 1 ]]; then
                echo "==> Downloading ${pkgfile}"
                # if this exists, it's corrupt
                rm -f ${cache_dir}/${pkgfile}
                if [[ ${HAVE_INTERNET} == "true" || -n ${PKGSRC_DIR} ]]; then
                    (cd ${cache_dir} \
                        && pkgin_get ${url} ${pkgfile} ./) \
                        || fatal "could not download ${url}/${pkgfile}"
                    [[ -n ${MD5} ]] && ACTUAL_MD5=$(openssl dgst -md5 ${cache_dir}/${pkgfile} | awk '{print $NF}')
                    [[ -n ${SHA512} ]] && ACTUAL_SHA512=$(openssl dgst -sha512 ${cache_dir}/${pkgfile} | awk '{print $NF}')
                    [[ -n ${SHA512} && ${SHA512} != ${ACTUAL_SHA512} ]] && fatal "corrupt download of ${pkgfile}"
                    [[ -n ${MD5} && ${MD5} != ${ACTUAL_MD5} ]] && fatal "corrupt download of ${pkgfile}"
                else
                    fatal "Need Internet to download ${pkgfile}"
                fi
            fi
        done

        mkdir -p ${STAGE}/extra/pkgsrc/${name}
        local pkg=
        for pkg in ${pkgs}; do
            ln ${cache_dir}/${pkg} ${STAGE}/extra/pkgsrc/${name}/${pkg}
        done
    done
}

function valid_tgz_archive
{
    filename=$1
    if [[ -f ${filename} ]] && ${TAR} -ztf ${filename} > /dev/null; then
        return 0
    else
        return 1
    fi
}

function valid_archive
{
    filename=$1
    if [[ -f ${filename} ]] && ${TAR} -tf ${filename} > /dev/null; then
        return 0
    else
        return 1
    fi
}

function cleanup_bit
{
    local bits_pattern=$1

    local bits_dir="${ROOT}/cache"
    local kept=0
    local keep_bits=$(build_spec keep-bits)

    if [[ -n ${keep_bits} && ${keep_bits} -gt 0 ]]; then
        echo "CLEANUP_BIT CALLED FOR: '${bits_pattern}'" >&2

        local bit=
        for bit in $(ls -1 ${bits_dir} | grep "${bits_pattern}" | sort -r); do
            if [[ ${kept} -lt ${keep_bits} ]]; then
                echo "KEEPING: ${bit}" >&2
                kept=$((${kept} + 1))
            else
                echo "DELETING: ${bit}" >&2
                rm ${bits_dir}/${bit} >&2
            fi
        done
    fi
}

# Get a bit (from BITS_URL/BITS_DIR/BITS_BRANCH) to the local "cache/" dir.
# If the file base name already exists in the cache dir, then it is not
# re-downloaded.
function get_bit
{
    # Presumption: This pattern is of the form:
    # (a) "$single-level-dir/$file-regex-pattern", or
    # (b) "$single-level-dir/$branch-override/$file-regex-pattern", or
    # (c) A complete URL.
    pattern=$1

    local get_bit_rv=

    local pattern_dir=$(dirname $pattern)
    local pattern_base="^$(basename $pattern)"
    if [[ ${pattern:0:7} == "http://" || ${pattern:0:8} == "https://" ]]; then
        # Bit "pattern" is a full URL, download that.
        local url=$pattern
        local file_name=$(basename $url)
        local cache_path=${ROOT}/cache/${file_name}
        local hostname=$(echo $url | cut -f3 -d "/" | cut -f1 -d ":")
        if [[ "$hostname" == "stuff.joyent.us" ]]; then
            echo "Adding auth info for stuff.joyent.us in bit URL." >&2
            if [[ ! -f "$HOME/.sdcbuild.json" ]]; then
                fatal "No '$HOME/.sdcbuild.json' with stuff.joyent.us auth info."
            fi
            local username=$(cat $HOME/.sdcbuild.json | ${ROOT}/bin/json username)
            local password=$(cat $HOME/.sdcbuild.json | ${ROOT}/bin/json password)
            url=$(echo $url | sed -e "s|://|://${username}:${password}@|")
        fi
        if [[ ! -f $cache_path ]]; then
            echo "Downloading '${url}' bit to cache." >&2
            (cd ${ROOT}/cache && curl ${CURL_OPTS} --fail \
                --connect-timeout 10 --progress-bar -k -O ${url} \
                || fatal "Unable to download '${url}'.")
        fi
        get_bit_rv=${cache_path}
    elif [[ ! -z "$BITS_DIR" ]]; then
        # Local BITS_DIR example:
        #   /home/jill/joy/mountain-gorilla/bits
        # where pattern='agentsshar/agents-master-*' is at:
        #   /home/jill/joy/mountain-gorilla/bits/agentsshar/agents-master-*
        local have_branch_override=$(dirname $pattern_dir)
        if [[ "$have_branch_override" != "." ]]; then
            # With a "BITS_DIR" the branch isn't used. Strip the
            # "branch_override" if given.
            pattern_dir=$(dirname $pattern_dir)
        fi
        local latest_name=$(ls -1 ${BITS_DIR}/${pattern_dir}/ \
            | grep "${pattern_base}" \
            | sort \
            | tail -1)
        if [[ -z "${latest_name}" ]]; then
            fatal "'${BITS_DIR}/${pattern}' did not match any files."
        fi
        local latest_path=${BITS_DIR}/${pattern_dir}/${latest_name}
        local cache_path=${ROOT}/cache/${latest_name}
        if [[ ! -f $cache_path ]]; then
            echo "Copying '${latest_path}' bit to cache." >&2
            cp ${latest_path} ${cache_path}
        fi
        get_bit_rv=${cache_path}
        cleanup_bit "${pattern_base}"
    elif [[ ${HAVE_INTERNET} == "true" ]]; then
        # BITS_URL URL example:
        #   https://user:pass@stuff.joyent.us/stuff/builds
        # where pattern='agentsshar/agents-master-*' is at:
        #   https://user:pass@stuff.joyent.us/stuff/builds/agentsshar/master-latest/agentsshar/agentsshar-master-*
        # where the "master" in "master-latest" is "BITS_BRANCH"
        # (or $branch_override).
        local branch=${BITS_BRANCH}
        local have_branch_override=$(dirname $pattern_dir)
        if [[ "$have_branch_override" != "." ]]; then
            branch=$(basename $pattern_dir)
            pattern_dir=$(dirname $pattern_dir)
        fi
        local url_dir="${BITS_URL}/${pattern_dir}/${branch}-latest/${pattern_dir}"
        local md5_url="${BITS_URL}/${pattern_dir}/${branch}-latest/md5sums.txt"
        local latest_name=$(curl ${CURL_OPTS} --fail -k -sS ${url_dir}/ \
            | grep "href=\"" \
            | cut -d'"' -f2 \
            | grep "${pattern_base}" \
            | sort \
            | tail -1)
        if [[ -z "${latest_name}" ]]; then
            fatal "Could not find '${pattern_base}' in '${url_dir}'."
        fi
        local correct_md5=$(curl --fail -k -sS ${md5_url} \
            | grep "./${pattern_dir}/${latest_name}$" | cut -d ' ' -f1)
        local cache_path=${ROOT}/cache/${latest_name}
        local ok=0
        local retries=0
        while [[ ${ok} -eq 0 && ${retries} -lt 3 ]]; do
            if [[ ! -f $cache_path ]]; then
                echo "Downloading '${url_dir}/${latest_name}' bit to cache." >&2
                (cd ${ROOT}/cache \
                    && curl ${CURL_OPTS} --fail --connect-timeout 10 --progress-bar -k \
                        -O ${url_dir}/${latest_name} \
                    || fatal "Unable to download '${url_dir}/${latest_name}'.")
            fi

            local my_md5=$(openssl dgst -md5 < ${cache_path} | awk '{print $NF}')
            if [[ ${my_md5} == ${correct_md5} ]]; then
                ok=1
                continue
            else
                rm -f ${cache_path}
                retries=$((${retries} + 1))
                echo "CORRUPT OR TRUNCATED ${cache_path}, deleted and trying again (attempt ${retries}/3)" >&2
            fi
        done
        if [[ ${ok} -ne 1 ]]; then
            fatal "Unable to get non-corrupt version of ${cache_path} after ${retries} attempts."
        fi
        get_bit_rv=${cache_path}
        cleanup_bit "${pattern_base}"
    else
        local latest_name=$(ls -1 ${ROOT}/cache/ \
            | grep "${pattern_base}" \
            | sort \
            | tail -1)
        if [[ -z "${latest_name}" ]]; then
            fatal "'${ROOT}/cache/${pattern}' did not match any files (and we have no Internet)."
        fi
        local latest_path=${ROOT}/cache/${latest_name}
        echo "Use '${latest_path}' bit already in cache (no Internet)." >&2
        get_bit_rv=${latest_path}
        cleanup_bit "${pattern}"
    fi
    echo "${get_bit_rv}"
}

function _check_vpn
{
    if [[ ${HAVE_INTERNET} == "true" ]]; then
        local host=${1##*//}
        ping -o -t 3 ${host} &> /dev/null
        local result=$?
        if [[ ${result} -ne 0 ]]; then
            echo "Can't ping ${host} (are you on the VPN?)"
            exit ${result}
        fi
    fi
}


function copy_platform
{
    [[ -z "${loops}" ]] && loops=0
    loops=$((${loops} + 1))
    [[ ${loops} -gt 2 ]] && fatal "Unable to download platform, looping!"

    # platform_file is optional, if specified, that platform will be used
    # instead of looking for the newest that matches platform-<release>
    local platform_file=$1
    local platform_release=$2

    local image
    if [[ -z ${platform_file} ]]; then
        [[ -z "${platform_release}" ]] \
            && fatal "Must define 'platform_file' or 'platform_release' " \
                     "for call to 'copy_platform()'."

        image=$(get_bit "platform/platform-${platform_release}-.*")

        if [[ -f ${image} ]] && ! valid_archive ${image}; then
            echo "Removing corrupt ${image}"
            rm -f ${image}
            image=
            # unset image and try again
            copy_platform "${platform_file}" "${platform_release}"
        fi
    else
        image=${platform_file}
        echo "==> Using ${image} as platform image"
        if ! valid_archive "${image}"; then
            fatal "Refusing to use corrupt platform ${image}"
        fi
    fi

    export USING_PLATFORM=${image}

    LIVEIMG_VERSION=`basename ${image} \
        | sed -e "s/platform.*-\([0-9TZ]*\)\.tgz/\1/"`

    echo "==> Unpacking `basename ${image}`"
    (set -e; cd ${STAGE}/; ${TAR} -zxf ${image}; mkdir -p os/${LIVEIMG_VERSION}; \
        mv platform-* os/${LIVEIMG_VERSION}/platform) \
        || fatal "Unable to unpack platform"
    if [[ -f ${STAGE}/os/${LIVEIMG_VERSION}/platform/root.password ]]; then
        (cd ${STAGE}/ \
            && mkdir -p private \
            && mv -f os/${LIVEIMG_VERSION}/platform/root.password \
                private/root.password.${LIVEIMG_VERSION}) \
            || fatal "Unable to move root.password"
    fi
    root_pw=$(cat ${STAGE}/private/root.password.${LIVEIMG_VERSION})
    echo "Root password is: '${root_pw}'"

    # Create the menu.lst file
    cat ${STAGE}/boot/grub/menu.lst.tmpl | sed \
        -e "s|/PLATFORM/|/os/${LIVEIMG_VERSION}/platform/|" \
        > ${STAGE}/boot/grub/menu.lst

    rm -f ${LOGDIR}/latest
    ln -s ${LOGFILE} ${LOGDIR}/latest

    loops=
}


# Copy the latest amon-agent-*.tgz package (using the 'agent-shar' branch
# setting in build.spec) into the staging area.
#
# Note: A separate extra/amon-agent dir is a placeholder until the amon-agent
# is in pkgsrc.
function copy_amon_agent
{
    [[ -z "${loops}" ]] && loops=0
    loops=$((${loops} + 1))
    [[ ${loops} -gt 2 ]] && fatal "Unable to download amon-agent, looping!"

    # See if there's a specific amon-agent branch we're supposed to use.
    local branch=$(build_spec agents-shar)
    if [[ -z ${branch} ]]; then
        branch="master"
    fi

    local amon_agent_path=$(get_bit "amon/amon-agent-${branch}-.*\.tgz")
    echo "Copying $(basename $amon_agent_path) to stage."
    mkdir -p ${STAGE}/extra/amon-agent
    cp ${amon_agent_path} ${STAGE}/extra/amon-agent/amon-agent.tgz

    loops=
}


function get_agentsshar
{
    [[ -z "${loops}" ]] && loops=0
    loops=$((${loops} + 1))
    [[ ${loops} -gt 2 ]] && fatal "Unable to download agents, looping!"

    local agentsshar_branch=$1

    if [[ -f ${agentsshar_branch} && -f ${agentsshar_branch/%sh/md5sum} ]]; then
        local agentsshar_path=${agentsshar_branch}
        local agentsmd5_path=${agentsshar_branch/%sh/md5sum}
    else
        local agentsshar_path=$(get_bit "agentsshar/agents-${agentsshar_branch}-.*\.sh")
        local agentsmd5_path=$(get_bit "agentsshar/agents-${agentsshar_branch}-.*\.md5sum")
    fi

    # Make sure it's not corrupt.
    local MD5=$(cat ${agentsmd5_path})
    local ACTUAL_MD5=$(openssl dgst -md5 ${agentsshar_path} | awk '{print $NF}')
    if [[ -z ${MD5} ]] \
        || [[ -z ${ACTUAL_MD5} ]] \
        || [[ ${MD5} != ${ACTUAL_MD5} ]]; then
        echo "Removing corrupt ${agentsshar_path}"
        rm -f ${agentsshar_path} ${agentsmd5_path}
        get_agentsshar ${agentsshar_branch}
    fi

    echo "Copying $(basename $agentsshar_path) to stage."
    mkdir -p ${STAGE}/ur-scripts
    cp ${agentsshar_path} ${STAGE}/ur-scripts/

    loops=
}

function copy_agentsshar
{
    # See if there's a specific agents shar we're supposed to use
    if [[ -z ${agentsshar_branch} ]]; then
        agentsshar_branch=$(build_spec agents-shar)
    fi

    if [[ -z ${agentsshar_branch} ]]; then
        agentsshar_branch="master"
    fi

    get_agentsshar ${agentsshar_branch}
}

function copy_datasets
{
    mkdir -p ${STAGE}/datasets
    mkdir -p ${ROOT}/datasets

    datasets_json=$(build_spec datasets)

    [[ -n ${datasets_json} ]] \
        || fatal "Unable to find datasets information in build.spec"

    num_datasets=$(echo "${datasets_json}" | ${ROOT}/bin/json length)
    index=0
    while [[ ${index} -lt ${num_datasets} ]]; do
        name=$(echo "${datasets_json}" | ${ROOT}/bin/json ${index}.name)
        uuid=$(echo "${datasets_json}" | ${ROOT}/bin/json ${index}.uuid)
        manifest_url=$(echo "${datasets_json}" | ${ROOT}/bin/json ${index}.manifest_url)
        file_url=$(echo "${datasets_json}" | ${ROOT}/bin/json ${index}.file_url)
        headnode_zones=$(echo "${datasets_json}" \
            | ${ROOT}/bin/json ${index}.headnode_zones)
        manifest="${ROOT}/cache/${name}.dsmanifest"

        if [[ ! -f $manifest ]] ; then
            if [[ -f ${ROOT}/datasets/${name}.dsmanifest ]]; then
                cp ${ROOT}/datasets/${name}.dsmanifest ${manifest}
            elif [[ ${HAVE_INTERNET} == "true" ]]; then
                echo "==> Downloading ${name} manifest"

                if [[ -z ${manifest_url} ]]; then
                    manifest_url="${DSAPI_URL}/datasets/${uuid}"
                fi
                (curl ${CURL_OPTS} \
                    -k -o ${manifest} ${manifest_url}) \
                    || fatal "Unable to download ${name} manifest"
            else
                fatal "Don't have required '${name}' manifest" \
                    "and can't download (no Internet)"
            fi
        fi

        if [[ -z ${file_url} ]]; then
            local path=$(cat ${manifest} | ${ROOT}/bin/json files[0].path)
            local uri=$(cat ${manifest} | ${ROOT}/bin/json files[0].url)
            if [[ -n $(echo "${uri}" | grep "datasets.joyent.com" 2>/dev/null) ]]; then
                # use proper credentials when talking to datasets.joyent.com
                file_url="${DSAPI_URL}/datasets/${uuid}/${path}"
            fi
        fi
        if [[ -z ${file_url} ]]; then
            fatal "Unable to find URL for ${name}"
        fi

        local sha1=$(cat ${manifest} | ${ROOT}/bin/json files[0].sha1)
        copy_dataset ${name} ${file_url} ${sha1}
        echo "==> Copying ${name} manifest"
        cp ${manifest} ${STAGE}/datasets/

        # Since create-zone.sh needs to know which dataset it should use to
        # base the headnode zones on, we create these files here, one which
        # contains the filename of the 'smartos' dataset and one that contains
        # its UUID.
        #
        # Note: ${dataset_file} is set by copy_dataset
        if [[ -n ${headnode_zones} && ${headnode_zones} == "true" ]]; then
            echo "${uuid}" > ${STAGE}/datasets/smartos.uuid
            echo "${dataset_file}" > ${STAGE}/datasets/smartos.filename
        fi

        index=$((${index} + 1))
    done
}

# This is temporary, until we have all the SDC datasets at the same place.
# It might be good anyway to add a little check to verify arguments are given.
function copy_dataset
{
  local dataset=$1
  local dataset_uri=$2
  local dataset_sha1=$3

  dataset_file=$(basename ${dataset_uri})
  if [ -e ${ROOT}/cache/${dataset_file} ]; then
    if [[ ${dataset_file} =~ gz$ ]]; then
        if ! gzip -t ${ROOT}/cache/${dataset_file}; then
            echo "==> Corrupt ${dataset_file}, deleting..."
            rm -f ${ROOT}/cache/${dataset_file}
        fi
    elif ! bzip2 -t ${ROOT}/cache/${dataset_file}; then
        echo "==> Corrupt ${dataset_file}, deleting..."
        rm -f ${ROOT}/cache/${dataset_file}
    fi
  fi

  if [[ ! -f ${ROOT}/cache/${dataset_file} ]]; then
      if [[ ${HAVE_INTERNET} == "true" ]]; then
          echo "==> Downloading ${dataset_file}"
          (cd ${ROOT}/cache && curl ${CURL_OPTS} -k \
              -O ${dataset_uri}) \
              || fatal "Unable to download ${dataset_file}"
      else
          fatal "Don't have Internet, and don't have valid " \
              "${dataset_file}. Can't build."
      fi
  fi

  local cached_dataset_sha1=$(${SUM} ${ROOT}/cache/${dataset_file} | awk '{print $1}')
  if [[ ${cached_dataset_sha1} != ${dataset_sha1} ]]; then
    rm -f ${ROOT}/cache/${dataset_file}
    fatal "Corrupt ${dataset_file} (doesn't match sha1 in manifest), deleted! Try build again."
  fi

  echo "==> Copying ${dataset_file}"
  ln ${ROOT}/cache/${dataset_file} ${STAGE}/datasets/${dataset_file}
}

# Get a smartdc zone's FS tarball.
#
# Usage:
#   get_fs_tarball TARGET DST-DIR
#
# Example:
#   get_fs_tarball 'ca/ca-pkg-master-.*.tar.bz2' cache/stage/zones/ca
#       Here we want to find/download the latest ca-pkg-... tarball and copy
#       it to cache/ca.tar.bz2 (where that "ca" is the basename of
#       "zones/ca").
#
#   get_fs_tarball 'https://stuff.joyent.us/stuff/builds/workflow/release-20120809-20120809T173325Z/workflow/workflow-pkg-release-20120809-20120809T173325Z-gea41bc8.tar.bz2' cache/stage/zones/workflow
#       Here we download the given URL, cache it to "cache/$basename" and
#       copy it to the DST-DIR.
#
#   get_fs_tarball /var/tmp/ca-pkg-master-1234.tar.bz2 cache/stage/zones/ca
#       The "TARGET" can be a path to an existing file to use.
#
function get_fs_tarball
{
    local target=$1
    local dst_dir=$2

    if [[ -z ${dst_dir} ]] || [[ ! -d ${dst_dir} ]]; then
        fatal "get_fs_tarball(): No destination dir specified or not a directory."
    fi

    [[ -z ${target} ]] && fatal "get_fs_tarball(): No target specified."

    # First get it and cache to cache/$zone.tar.bz2. Then we'll copy
    # it to $dst_dir.
    zone=$(basename ${dst_dir})
    cache_path=${ROOT}/cache/${zone}.tar.bz2
    if [[ -f ${target} ]]; then
        # if this is the filename of an existing file, we'll use that
        if [[ ${target} != "${cache_path}" ]]; then
            echo "$(basename ${target} .tar.bz2)" > ${dst_dir}/stamp
            cp ${target} ${cache_path}
        fi
    else
        # not a file so assume it's a pattern (find the latest) or URL
        local bit_cache_path=$(get_bit ${target})
        rm -f ${cache_path}
        echo "$(basename ${bit_cache_path} .tar.bz2)" > ${dst_dir}/stamp
        ln ${bit_cache_path} ${cache_path}
    fi

    # Validate and copy
    if [[ ! -f ${cache_path} ]]; then
        fatal "Unable to get file '${cache_path}'."
    elif ! bzip2 -t ${cache_path}; then
        fatal "Corrupt file ${cache_path}, please delete or fix and try again."
    else
        cp ${cache_path} ${dst_dir}/fs.tar.bz2
    fi
}


# Get the image manifest and file for the given core SDC zone.
#
# Usage:
#   get_core_zone_image ZONE TARGET DST-DIR
#
# where:
# - "TARGET" is either a full path to an existing image manifest file
#   or (more typically) a pattern used by the "get_bit" function for getting
#   a manifest file from the MG builds area on stuff.joyent.us.
#
# Examples:
#   get_core_zone_image admin "adminui/adminui-zfs-.*manifest" \
#           /home/trent/usb-headnode/cache/stage/datasets
#
function get_core_zone_image
{
    local zone=$1
    local target=$2
    local dst_dir=$3

    if [[ -z "${target}" ]]; then
        fatal "get_core_zone_image(): empty target for zone '$zone' (Is '$zone-image' set in build.spec?)"
    fi
    if [[ -z "${dst_dir}" ]] || [[ ! -d ${dst_dir} ]]; then
        fatal "get_core_zone_image(): No destination dir specified or not a directory."
    fi

    # First get it and cache it. Then we'll copy it to $dst_dir.
    local manifest_base dst_manifest dst_file
    if [[ -f ${target} ]]; then
        # Existing local file. Use that.
        # XXX Normalize to '.imgmanifest' ext, which will be the eventual
        #     usage but is '.dsmanifest' in the builds at this time.
        manifest_base=$(basename $target)
        dst_manifest=${dst_dir}/${manifest_base%.*}.imgmanifest
        if [[ "${target}" != "${dst_manifest}" ]]; then
            ln ${target} ${dst_manifest}
        fi
        local src_file=$(ls -1 ${target%.*}* | grep -v 'manifest$')
        dst_file=${dst_dir}/$(basename $src_file)
        ln ${src_file} ${dst_file}
    else
        # Not a local file, this is a pattern for 'get_bit'.
        local bit_cache_path=$(get_bit ${target})
        # XXX Normalize to '.imgmanifest' ext, which will be the eventual
        #     usage but is '.dsmanifest' in the builds at this time.
        manifest_base=$(basename $bit_cache_path)
        dst_manifest=${dst_dir}/${manifest_base%.*}.imgmanifest
        rm -f ${dst_manifest}
        ln ${bit_cache_path} ${dst_manifest}
        # Example of getting get_bit pattern for the image *file*:
        #   target 'adminui/adminui-zfs.*manifest'
        #   bit_cache_path '/.../cache/adminui-zfs-master-20130401T104924Z-g1695958.zfs.imgmanifest'
        #   -> file_pattern 'adminui/adminui-zfs-master-20130401T104924Z-g1695958.zfs.gz'
        # Note: We are hardcoding that sdc images use gzip compression here.
        local file_pattern=${target%/*}/${manifest_base%.*manifest}.gz
        bit_cache_path=$(get_bit $file_pattern)
        dst_file=${dst_dir}/$(basename $bit_cache_path)
        rm -f ${dst_file}
        ln ${bit_cache_path} ${dst_file}
        gzip -t ${dst_file} \
            || fatal "Corrupt ${dst_file}, please remove and re-run."
    fi

    # HEAD-1507 - this should be temporary until we get proto-SAPI returning
    # a payload for vmadm. This should allow headnode.sh:create_zone()
    # and build_payload to handle the sdc images with less trouble until
    # then.
    echo "NO_FS_TARBALL=\"true\"" >> ${STAGE}/zones/${zone}/zoneconfig
    # PCFS, sigh.
    echo $(basename $dst_manifest) | tr [:upper:] [:lower:] > ${STAGE}/zones/${zone}/dataset
}

function copy_zones
{
    if [[ -n $ZONE_DIR ]]; then
        export ADMINUI_DIR=${ZONE_DIR}/mcp_api_admin
        export BOOTER_DIR=${ZONE_DIR}/booter
        export MAPI_DIR=${ZONE_DIR}/mcp_api_gateway
        export PORTAL_DIR=${ZONE_DIR}/public-web-client
        export CLOUDAPI_DIR=${ZONE_DIR}/cloud-api
        export USAGEAPI_DIR=${ZONE_DIR}/usage_api
        export UFDS_DIR=${ZONE_DIR}/ufds
        export WORKFLOW_DIR=${ZONE_DIR}/workflow
        export BINDER_DIR=${ZONE_DIR}/binder
        export VMAPI_DIR=${ZONE_DIR}/vmapi
        export DAPI_DIR=${ZONE_DIR}/dapi
        export CNAPI_DIR=${ZONE_DIR}/cnapi
        export NAPI_DIR=${ZONE_DIR}/napi
        export MANATEE_DIR=${ZONE_DIR}/manatee
        export MORAY_DIR=${ZONE_DIR}/moray
    fi

    for zone in $(ls ${STAGE}/zones); do

        # Symlinks aren't supported on pcfs, so we copy the files
        if [[ -L ${ROOT}/zones/${zone}/backup ]]; then
            rm ${STAGE}/zones/${zone}/backup
            cp ${ROOT}/zones/${zone}/backup ${STAGE}/zones/${zone}/backup
        fi
        if [[ -L ${ROOT}/zones/${zone}/restore ]]; then
            rm ${STAGE}/zones/${zone}/restore
            cp ${ROOT}/zones/${zone}/restore ${STAGE}/zones/${zone}/restore
        fi

        if [[ -n ${USE_IMAGES} ]]; then
            if [[ ${zone} == "manta" ]]; then
                echo "Skipping manta tools zone - MANTA-1071"
                continue
            fi
            local image_pattern=$(build_spec ${zone}-image)
            mkdir -p ${STAGE}/datasets
            get_core_zone_image "${zone}" "${image_pattern}" ${STAGE}/datasets
        else
            local tarball_pattern=$(build_spec ${zone}-tarball)
            if [[ -n ${tarball_pattern} ]]; then
                # Find latest that matches tarball_pattern.
                get_fs_tarball ${tarball_pattern} ${STAGE}/zones/${zone}
            fi
            if [[ ! -f ${STAGE}/zones/${zone}/fs.tar.bz2 ]]; then
                fatal "Unable to find fs.tar.bz2 for ${zone}"
            fi
        fi
    done
}

# Copy in the appropriate version of registrar using (if applicable) the
# registrar branch specified in build.spec.
# HEAD-1507 obsolete function.
function copy_registrar
{
    [[ -z ${USE_IMAGES} ]] || return 0

    [[ -z "${loops}" ]] && loops=0
    loops=$((${loops} + 1))
    [[ ${loops} -gt 2 ]] && fatal "Unable to download registrar, looping!"

    # See if there's a specific registrar branch we're supposed to use.
    local branch=$(build_spec registrar)
    if [[ -z ${branch} ]]; then
        branch="master"
    fi

    local registrar_path=$(get_bit "registrar/registrar-pkg-${branch}-.*\.bz2")
    echo "Copying $(basename $registrar_path) to stage."
    mkdir -p ${STAGE}/extra/registrar
    cp ${registrar_path} ${STAGE}/extra/registrar/registrar.bz2

    loops=
}


function copy_tools
{
    if [[ -d ${ROOT}/tools ]]; then
        cp -r ${ROOT}/tools ${STAGE}/tools
    fi

    if [[ -d ${ROOT}/tools-modules ]]; then
        (cd ${ROOT}/tools-modules && ${TAR} -cf ${STAGE}/tools-modules.tar ./)
    fi

    if [[ -x ${RONNJS} ]]; then
        for file in $(find ${ROOT}/tools-man/ -name "*.ronn"); do
            ${RONNJS} --roff --build ${file} --date `git log -1 --date=short --pretty=format:'%cd' ${file}` `date +%Y`
        done
    else
        fatal "unable to build man pages, missing: ${RONNJS}"
    fi

    mkdir -p ${STAGE}/tools-man
    for file in $(cd ${ROOT}/tools-man && find . -type f -name "*.roff"); do
        mkdir -p ${STAGE}/tools-man/$(dirname ${file})
        cp ${ROOT}/tools-man/${file} ${STAGE}/tools-man/$(dirname ${file})/$(basename ${file} .roff)
    done

    # HEAD-1057/MANTA-1071 to be cleaned up.
    local target=$(build_spec manta-tools)
    local bit_cache_path=$(get_bit ${target})
    cp ${bit_cache_path} ${ROOT}/cache/manta.tar.bz2
    mkdir -p ${STAGE}/extra/manta
    cp ${ROOT}/cache/manta.tar.bz2 ${STAGE}/extra/manta

    target=$(build_spec imgapi-cli-tarball)
    bit_cache_path=$(get_bit ${target})
    cp ${bit_cache_path} ${ROOT}/cache/imgapi-cli.tar.bz2
    mkdir -p ${STAGE}/extra/imgapi-cli
    cp ${ROOT}/cache/imgapi-cli.tar.bz2 ${STAGE}/extra/imgapi-cli
}

function copy_to_mount
{
    echo "${THIS_BUILDSTAMP}" > ${STAGE}/version

    (cd ${STAGE} && ${TAR} ${TAR_ROOT} -cf - ./) \
        | (cd ${MNT_DIR} && ${SUCMD} ${TAR} --no-same-owner -xvf -) \
        || fatal "Unable to copy files to mount"
}

function copy_agents65
{
    local bit_cache_path
    mkdir -p ${STAGE}/agents65
    bit_cache_path=$(get_bit "agents-upgrade/release-20110901-upgrade/heartbeater-*")
    cp $bit_cache_path ${STAGE}/agents65/heartbeater-65.tgz
    bit_cache_path=$(get_bit "agents-upgrade/release-20110901-upgrade/provisioner-v2-*")
    cp $bit_cache_path ${STAGE}/agents65/provisioner-v2-65.tgz
}

function add_manifests
{
    # build manifest of USB files + move in boot_archive manifest
    rm -f $STAGE/usb_key.manifest || true
    (cd ${STAGE}/ \
        && find . -type f -exec openssl dgst -md5 {} \; | awk '{print $NF}') \
        > $STAGE/usb_key.manifest
    [[ $? -eq 0 ]] || fatal "Unable to add manifests"
    rm -f $STAGE/boot_archive.manifest || true

    cp ${STAGE}/os/${LIVEIMG_VERSION}/platform/i86pc/amd64/boot_archive.manifest \
        $STAGE/boot_archive.manifest
    chmod 444 $STAGE/*.manifest
}

# Main()

check_nodejs
test_rootperms

create_directories
load_buildspec
copy_base
generate_grub_menu
copy_pkgsrc
copy_amon_agent
copy_platform "${PLATFORM_FILE}" "${PLATFORM_RELEASE}"
copy_agentsshar
copy_agents65
copy_datasets
copy_zones
copy_registrar
copy_tools
copy_config

unpack_image
add_manifests
mount_image
trap 'cleanup' EXIT
copy_to_mount
cleanup
create_output

# Unfortunately the log contains a whole bunch of progress updates,
# clean that up.
if [[ -f ${LOGFILE} ]]; then
    cat ${LOGFILE} | ${GREP} -v "
" > ${LOGFILE}.tmp \
    && mv ${LOGFILE}.tmp ${LOGFILE}
fi

if [ ${ERROR} -ne 0 ]; then
    fatal "==> SOMETHING WENT WRONG! ERROR: ${ERROR}"
fi

echo "==> DONE"

exit 0
